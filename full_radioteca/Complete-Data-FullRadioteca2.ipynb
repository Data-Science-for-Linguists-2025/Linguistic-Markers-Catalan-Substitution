{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "059448c8-5573-4188-9344-2b9c72caa1e0",
   "metadata": {},
   "source": [
    "# Complete Dataframe with the full Radioteca"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658e0283-9bca-447f-ae70-ac057a03678e",
   "metadata": {},
   "source": [
    "This notebook is the second part of the creation of the dataframe containing all the data from the different corpora adquired. The separate corpora were turned into a dataframes and explored in [Data-Parsing-Exploratory-Analysis-Part1](https://github.com/Data-Science-for-Linguists-2025/Linguistic-Markers-Catalan-Substitution/blob/main/Data-Parsing-Exploratory-Analysis-Part1.ipynb.) individually. In this notebook they are merged following the same steps as in our first approach without the full radioteca except when dealing with radioteca."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fec59667-f472-4199-92d6-0c83edf7fd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries and tools\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1822d23a-db14-4722-9608-c499b2b0f983",
   "metadata": {},
   "source": [
    "Some of the data frames come with a great amount of metadata that won't be necessary for our research purposes. To keep it we will maintain the separate dataframes and create a merged one extracting only the columns necessary for our research question. That will be the title, date and text/content. We will also create other columns that will be useful for our Exploratory Data Analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffb14518-2b44-482c-bc5e-398e4bfd028b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the individual dataframes from their pickled form\n",
    "CTILC_df = pd.read_pickle(\"Data/CTILC.pkl\")\n",
    "parlament_parla_df = pd.read_pickle(\"Data/parlaparla.pkl\")\n",
    "parlaMint_df = pd.read_pickle(\"Data/parlaMint.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54372918-c0cb-48ce-b29a-f1cb7ee0833b",
   "metadata": {},
   "source": [
    "**Comment:** Only the first three dataframes CTILC, Parlament Parla and Parlamint have been loaded as we will treat radioteca differently because of its huge dimensions. We will load the data in chuncks instead of all at once as otherwise it would make the notebook crash. Moreover, as this notebook is the second take of the merging of the dataframes, as previously we had only scraped part of radioteca, and process was re-done after scraping the enirety of radioteca df, we already know it will give us a very imbalenced distribution of the data. It is a massive dataframe but it will only bring in lots of data about the same years, which is not what we are looking for. Therefore, we will perform 2 strategies to reduce the amount of data and create a sample of the whole radioteca dataframe while trying to avoid any biases. \n",
    "\n",
    "The strategies, as in the original notebook [Data-Parsing-Exploratory-Analysis-2](https://github.com/Data-Science-for-Linguists-2025/Linguistic-Markers-Catalan-Substitution/blob/main/Data-Parsing-Exploratory-Analysis-2.ipynb), will be:\\\n",
    "**Strategy 1:Limiting Individual Contribution**\\\n",
    "Aiming for the most unbiased and representative data, we will first drop individual Speaker's contributions if they are over 1500 characters (about 200 words) long for the Radioteca data. This will allow for a less speaker-specific analysis.\\\n",
    "**Strategy 2: Balancing Program  Contribution**\\\n",
    "Since Radioteca has a lot of metadata, we will also use another piece of data ensure a more representative distribution while reducing the data size by limiting a show's episode contribution to 1500 characters as well. That way, we are also ensuring a more diverse corpora topic-wise and less show-specific data.\n",
    "\n",
    "We will do it without loading the full dataset completely, year file per year file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66db7d4b-2216-4b0e-957d-e488f9b70782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for strategy 1, speaker limit (we will reuse it for the other larger dataframes\n",
    "def under1500(dataframe, contrivutorColName):\n",
    "    '''\n",
    "    takes in a dataframe\n",
    "    after the cumulative text length of one speaker goes over 1500 characters\n",
    "    the following contributions are no longer added to the dataframe\n",
    "    ensuring a max of 1500 characters per speaker/contributor\n",
    "    prints out the maximum and minimum length per contributor before and after the change\n",
    "    '''\n",
    "    if contrivutorColName == \"Speaker\":\n",
    "        dataframe[\"CUMSUM_len\"] = dataframe.groupby([\"Episode\", contrivutorColName])[\"Text_len\"].cumsum()\n",
    "    else:\n",
    "        dataframe[\"CUMSUM_len\"] = dataframe.groupby(contrivutorColName)[\"Text_len\"].cumsum()\n",
    "    max1500_df = dataframe[dataframe[\"CUMSUM_len\"] <= 1500]\n",
    "    if contrivutorColName == \"Speaker\":\n",
    "        before_spkcount = dataframe.groupby([\"Episode\", contrivutorColName])[\"Text_len\"].sum()\n",
    "        after_spkcount = max1500_df.groupby([\"Episode\", contrivutorColName])[\"Text_len\"].sum()\n",
    "    else:\n",
    "        before_spkcount = dataframe.groupby(contrivutorColName)[\"Text_len\"].sum()\n",
    "        after_spkcount = max1500_df.groupby(contrivutorColName)[\"Text_len\"].sum()\n",
    "\n",
    "    #printing contribution max and min to ensure the process worked out well\n",
    "    # print(\"before:\", before_spkcount.max(), \"-\", before_spkcount.min(), \"after:\", after_spkcount.max(), \"-\", after_spkcount.min())\n",
    "    return max1500_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77bfb58-f60b-4d38-ad7b-f90868fd69f7",
   "metadata": {},
   "source": [
    "**Comment:** In the following code we are loading the the year files, as the dataframe was saved in several parquet files per year.\\\n",
    "We are creating a list with all of the radioteca_year.parquet files. Then, for each of the files, we are looking for the year in the file name so we can use it when we save it. For the data-reducing strategies, then, we first use the previously defined under1500 function to limit speaker contributions to 1500 tokens per speaker. Then we grouping the texts per episode, and when an episode's contributions goes over 1500 characters it is no longer added to the dataframe. Lastly we are turning the reduced dataframe, after applying the two strategies into a new parquet file in our data folder. We are also printing out the changes in the number of character maximums per year file, before and after the reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e54aac0-5eba-4c51-9c63-f57e4f69ff49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "radioteca_year_2004_reduced.parquet\n",
      "radioteca_year_1986_reduced.parquet\n",
      "radioteca_year_2003_reduced.parquet\n",
      "radioteca_year_2020_reduced.parquet\n",
      "radioteca_year_2013_reduced.parquet\n",
      "radioteca_year_2025_reduced.parquet\n",
      "radioteca_year_2012_reduced.parquet\n",
      "radioteca_year_1992_reduced.parquet\n",
      "radioteca_year_1995_reduced.parquet\n",
      "radioteca_year_2024_reduced.parquet\n",
      "radioteca_year_2015_reduced.parquet\n",
      "radioteca_year_2003_reduced.parquet\n",
      "radioteca_year_1987_reduced.parquet\n",
      "radioteca_year_2012_reduced.parquet\n",
      "radioteca_year_2002_reduced.parquet\n",
      "radioteca_year_1994_reduced.parquet\n",
      "radioteca_year_2014_reduced.parquet\n",
      "radioteca_year_2019_reduced.parquet\n",
      "radioteca_year_2009_reduced.parquet\n",
      "radioteca_year_2002_reduced.parquet\n",
      "radioteca_year_2000_reduced.parquet\n",
      "radioteca_year_2010_reduced.parquet\n",
      "radioteca_year_1995_reduced.parquet\n",
      "radioteca_year_2005_reduced.parquet\n",
      "radioteca_year_2011_reduced.parquet\n",
      "radioteca_year_2001_reduced.parquet\n",
      "radioteca_year_1994_reduced.parquet\n",
      "radioteca_year_2021_reduced.parquet\n",
      "radioteca_year_2013_reduced.parquet\n",
      "radioteca_year_1993_reduced.parquet\n",
      "radioteca_year_2008_reduced.parquet\n",
      "radioteca_year_2018_reduced.parquet\n",
      "radioteca_year_1998_reduced.parquet\n",
      "radioteca_year_2016_reduced.parquet\n",
      "radioteca_year_2022_reduced.parquet\n",
      "radioteca_year_1991_reduced.parquet\n",
      "radioteca_year_2000_reduced.parquet\n",
      "radioteca_year_2004_reduced.parquet\n",
      "radioteca_year_2014_reduced.parquet\n",
      "radioteca_year_1999_reduced.parquet\n",
      "radioteca_year_2019_reduced.parquet\n",
      "radioteca_year_2024_reduced.parquet\n",
      "radioteca_year_2023_reduced.parquet\n",
      "radioteca_year_1990_reduced.parquet\n",
      "radioteca_year_2015_reduced.parquet\n",
      "radioteca_year_2005_reduced.parquet\n",
      "radioteca_year_1987_reduced.parquet\n",
      "radioteca_year_2007_reduced.parquet\n",
      "radioteca_year_1991_reduced.parquet\n",
      "radioteca_year_2008_reduced.parquet\n",
      "radioteca_year_1999_reduced.parquet\n",
      "radioteca_year_2011_reduced.parquet\n",
      "radioteca_year_2023_reduced.parquet\n",
      "radioteca_year_1986_reduced.parquet\n",
      "radioteca_year_2022_reduced.parquet\n",
      "radioteca_year_2007_reduced.parquet\n",
      "radioteca_year_2017_reduced.parquet\n",
      "radioteca_year_1992_reduced.parquet\n",
      "radioteca_year_2006_reduced.parquet\n",
      "radioteca_year_2009_reduced.parquet\n",
      "radioteca_year_1990_reduced.parquet\n",
      "radioteca_year_2021_reduced.parquet\n",
      "radioteca_year_2010_reduced.parquet\n",
      "radioteca_year_2017_reduced.parquet\n",
      "radioteca_year_2020_reduced.parquet\n",
      "radioteca_year_1998_reduced.parquet\n",
      "radioteca_year_2016_reduced.parquet\n",
      "radioteca_year_2006_reduced.parquet\n",
      "radioteca_year_2001_reduced.parquet\n",
      "radioteca_year_1993_reduced.parquet\n",
      "radioteca_year_2018_reduced.parquet\n",
      "radioteca_year_2025_reduced.parquet\n"
     ]
    }
   ],
   "source": [
    "import glob as glob\n",
    "from tqdm import tqdm \n",
    "\n",
    "results = []\n",
    "files_per_year = glob.glob(\"data/radioteca_*.parquet\")\n",
    "\n",
    "for f in files_per_year:\n",
    "    year_pattern = re.compile(r\"\\d+\")\n",
    "    year = re.search(year_pattern, f).group()\n",
    "    radioteca_Ydf = pd.read_parquet(f)\n",
    "\n",
    "    print(f\"radioteca_year_{year}_reduced.parquet\")\n",
    "    \n",
    "    # Strategy 1: limit speaker contributions to 1500 characters\n",
    "    radioteca_Ydf = under1500(radioteca_Ydf, \"Speaker\")\n",
    "\n",
    "    # Strategy 2: limit episode contributions to 1500 characters\n",
    "    radioteca_Ydf[\"CUMSUM_ep\"] = radioteca_Ydf.groupby(\"Episode\")[\"Text_len\"].cumsum()\n",
    "    radioteca_Ydf = radioteca_Ydf[radioteca_Ydf[\"CUMSUM_ep\"] <= 1500]\n",
    "\n",
    "    radioteca_Ydf.to_parquet(f\"data/radioteca_year_{year}_reduced.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac098c24-3c9b-4319-b7ff-f3f1e1df458c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting all the reduced files paths\n",
    "reduced_files = glob.glob(\"data/radioteca_*_reduced.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de4617b5-6191-4ed1-9fa1-11a0a2e8a16e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/radioteca_year_2004_reduced.parquet\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# printing the first one to take a look\n",
    "print(reduced_files[0])\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf61211-2489-4fc9-a535-f4b5256ea011",
   "metadata": {},
   "source": [
    "Next we will concatenate all of the reduced dataframes from the parquets by appending them to a list and then concatenating them to a newly creadted radioteca_reduced dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "feba52ff-495c-4599-b924-432b2ee1e09a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 36/36 [00:01<00:00, 19.70it/s]\n"
     ]
    }
   ],
   "source": [
    "radioteca_reduced = pd.DataFrame() # creating the dataframe\n",
    "\n",
    "data_per_year = []\n",
    "\n",
    "for file in tqdm(reduced_files):\n",
    "    year_df = pd.read_parquet(file)\n",
    "    data_per_year.append(year_df)\n",
    "\n",
    "df = pd.concat(data_per_year, ignore_index=True)\n",
    "del data_per_year "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2fd46fbd-d10c-478a-aed6-d63314a5f7a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1648634 entries, 0 to 1648633\n",
      "Data columns (total 13 columns):\n",
      " #   Column      Non-Null Count    Dtype         \n",
      "---  ------      --------------    -----         \n",
      " 0   Line_id     1648634 non-null  object        \n",
      " 1   Speaker     1648634 non-null  object        \n",
      " 2   Time        1648634 non-null  object        \n",
      " 3   Text        1648634 non-null  object        \n",
      " 4   Date        1648634 non-null  datetime64[ns]\n",
      " 5   Station     1641081 non-null  object        \n",
      " 6   Show        1648634 non-null  object        \n",
      " 7   Episode     1648634 non-null  object        \n",
      " 8   URL         1648634 non-null  object        \n",
      " 9   Year        1648634 non-null  int32         \n",
      " 10  Text_len    1648634 non-null  int64         \n",
      " 11  CUMSUM_len  1648634 non-null  int64         \n",
      " 12  CUMSUM_ep   1648634 non-null  int64         \n",
      "dtypes: datetime64[ns](1), int32(1), int64(3), object(8)\n",
      "memory usage: 157.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f48cb1c-02c5-46db-b26a-765c4b0bdcc1",
   "metadata": {},
   "source": [
    "**Comment:** The data is much smaller and a bit more manageable now. It dropped from GBs of memory usage to a 157MB."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6093d1-7259-4fa7-9fdd-a51161806bf7",
   "metadata": {},
   "source": [
    "As mentioned previously we don't need a lot of the metadata we are storing in the dataframes, therefore we will just keep the columns we need. That is Year, ID, Text and Text_length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "675aa485-f7d7-4675-b077-840256f86082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a reduced dataframe with only the date info, title of the file, content/text columns and its length\n",
    "reduced_CTILC = CTILC_df.filter([\"Year\", \"Title\", \"Text\", \"Text_len\"])\n",
    "reduced_parlament_parla = parlament_parla_df.filter([\"Path\", \"Sentence\", \"Text\", \"Text_len\"])\n",
    "reduced_parlamint = parlaMint_df.filter([\"Date\", \"Title\", \"Text\", \"Text_len\"])\n",
    "reduced_radioteca = df.filter([\"Year\", \"Line_id\", \"Text\", \"Text_len\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4853c7f-f86f-44d7-ad26-5ec2254e4795",
   "metadata": {},
   "source": [
    "As we will be next combining all dataframes we will create a source column to avoid loosing the data of where the row came from after merging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e407b63a-60cb-44db-9011-07622ebaa730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding a Source Corpora column to keep the data\n",
    "reduced_CTILC[\"Source_corpora\"] = \"CTILC\"\n",
    "reduced_parlament_parla[\"Source_corpora\"] = \"Parlament Parla\"\n",
    "reduced_parlamint[\"Source_corpora\"] = \"ParlaMint\"\n",
    "reduced_radioteca[\"Source_corpora\"] = \"Radioteca.cat\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93009d6d-8d87-42d6-ad96-597df00de82a",
   "metadata": {},
   "source": [
    "We will only keep the year infomation as the date in the corpora parlamint and the radioteca data frame where the full (month, day, year) information was provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "506e48ed-7345-4ecd-a010-9560864cf0b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.int32'>\n",
      "<class 'numpy.int64'>\n"
     ]
    }
   ],
   "source": [
    "# casting year as integer\n",
    "print(type(reduced_radioteca[\"Year\"][0])) # it is currently a float that we extracted from the complete date\n",
    "reduced_radioteca[\"Year\"] = reduced_radioteca[\"Year\"].apply(int)\n",
    "print(type(reduced_radioteca[\"Year\"][0])) # checking if data type is now what we need"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff30674-e144-4a25-af13-02d57530ba81",
   "metadata": {},
   "source": [
    "There is no date metadata provided for the parlament parla dataframe.\\\n",
    "For our purposes, as we need a date we will make an aproximation based off the data given by the owners.\\\n",
    "Initially, on the original notebook, as Parlament Parla is presented as a corpora containing data from 2007 to 2015 I assigned the date aproximation 2010 to all the data in the Parlament Parla dataframe. However, that made the data cluster all on the same year which resulted in an even bigger imbalance in our data. Therefore, we will go straight into the second approach I took to date this dataframe. That is, distributing the parlament-parla files evenly throughout the years it covers (2007-2018) instead of approximating all of them as 2015.\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d1c97f0a-e0f4-4232-8c50-78767bcb116c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Path</th>\n",
       "      <th>Text</th>\n",
       "      <th>Text_len</th>\n",
       "      <th>Source_corpora</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>clean_train/3/1/31ca4d158eaef166c37a_18.87_23....</td>\n",
       "      <td>perquè que el president de catalunya sigui reb...</td>\n",
       "      <td>85</td>\n",
       "      <td>Parlament Parla</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Path  \\\n",
       "0  clean_train/3/1/31ca4d158eaef166c37a_18.87_23....   \n",
       "\n",
       "                                                Text  Text_len  \\\n",
       "0  perquè que el president de catalunya sigui reb...        85   \n",
       "\n",
       "    Source_corpora  \n",
       "0  Parlament Parla  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduced_parlament_parla.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31dc35d6-a7dd-43d3-9211-77d29031fe7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87356\n"
     ]
    }
   ],
   "source": [
    "# stablishing number of rows/documents/contributions that parlament parla contains \n",
    "size = reduced_parlament_parla.shape[0]\n",
    "print(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2f19357-086e-4648-ab98-c78cc9541651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a numpy array containing 3849 random years between 2007 to 2018, both included\n",
    "import numpy as np\n",
    "years = np.random.randint(2007, 2019, size) # 2019 will be excluded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "002c31ae-324a-4b6e-8061-caa74165aec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turning the numpy array into a list\n",
    "reduced_parlament_parla[\"Year\"] = years.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ebe30ec1-91ba-425b-8782-5d1c15d4d5dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year\n",
      "2011    7488\n",
      "2017    7357\n",
      "2014    7340\n",
      "2009    7321\n",
      "2008    7298\n",
      "2012    7293\n",
      "2007    7286\n",
      "2015    7222\n",
      "2013    7212\n",
      "2010    7188\n",
      "2018    7178\n",
      "2016    7173\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Total:\n",
      "87356\n",
      "\n",
      "Mean:\n",
      "7279.666666666667\n",
      "\n",
      "Standard deviation:\n",
      "92.10007732040317\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# checking the documents per year distribution we ended up with after the random year assignantion \n",
    "print(reduced_parlament_parla[\"Year\"].value_counts())\n",
    "print()\n",
    "print(\"Total:\")\n",
    "print(reduced_parlament_parla[\"Year\"].value_counts().sum())\n",
    "print()\n",
    "print(\"Mean:\")\n",
    "print(reduced_parlament_parla[\"Year\"].value_counts().mean())\n",
    "print()\n",
    "print(\"Standard deviation:\")\n",
    "print(reduced_parlament_parla[\"Year\"].value_counts().std())\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6fc097-a43f-41ad-aa0a-1c3a43bdb397",
   "metadata": {},
   "source": [
    "**Comment:**\\\n",
    "We ended up with a decently even spread through the years. All the years are represented with around 320 pieces of data.\n",
    "A standard deviation of 15 contributions of data, considering that we are working with a total of 3849, seems to be a reasonably uniform distribution that we can use, being mindful that it is not the real date of the documents and just an approximation. We should keep the data from 2007 to 2018 as grouped as possible to avoid making misanalyses or drawing incorrect conclusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5b88e7-2f0b-42d1-8625-1992a2aaa7d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dfd47169-451f-4c99-a36f-76a9de106862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjusting column names to match before concatenating\n",
    "reduced_CTILC = reduced_CTILC.rename(columns ={\"Year\":\"Year\", \"Title\":\"Line_id\", \"Text\":\"Text\", \n",
    "                                                                   \"Source_corpora\":\"Source_corpora\", \"Text_len\":\"Text_len\"})\n",
    "reduced_parlament_parla = reduced_parlament_parla.rename(columns ={\"Year\":\"Year\", \"Path\":\"Line_id\", \"Text\":\"Text\", \n",
    "                                                                   \"Source_corpora\":\"Source_corpora\", \"Text_len\":\"Text_len\"})\n",
    "reduced_parlamint = reduced_parlamint.rename(columns ={\"Date\":\"Year\", \"Title\":\"Line_id\", \"Text\":\"Text\", \n",
    "                                                       \"Source_corpora\":\"Source_corpora\", \"Text_len\":\"Text_len\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c50e2b79-95c2-4eff-9d05-a0ebc833c406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenating all datasets' relevant columns in a single data frame\n",
    "complete_data = pd.concat([reduced_CTILC, reduced_parlament_parla, reduced_parlamint, reduced_radioteca]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c830de0d-b8fe-4326-a273-0e942831c354",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Line_id</th>\n",
       "      <th>Text</th>\n",
       "      <th>Text_len</th>\n",
       "      <th>Source_corpora</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1926</td>\n",
       "      <td>Discurs llegit per... donar a conèxer la perso...</td>\n",
       "      <td>L'home que per amor al estudi, impulsat per un...</td>\n",
       "      <td>37497</td>\n",
       "      <td>CTILC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1920</td>\n",
       "      <td>Parlament llegit en la festa inaugural de l'Or...</td>\n",
       "      <td>Cantaires de la Garriga, Senyores i senyors:\\n...</td>\n",
       "      <td>9253</td>\n",
       "      <td>CTILC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1900</td>\n",
       "      <td>Discurs-pròlec</td>\n",
       "      <td>Discurs-prolec Llegit en la societat mèdic-far...</td>\n",
       "      <td>73881</td>\n",
       "      <td>CTILC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1894</td>\n",
       "      <td>Discurs</td>\n",
       "      <td>Senyors excelentissims, senyors:\\n\\nQuan rebí ...</td>\n",
       "      <td>29393</td>\n",
       "      <td>CTILC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1903</td>\n",
       "      <td>Discurs</td>\n",
       "      <td>Senyors:\\n\\nSembla que era air, y fa ja uns qu...</td>\n",
       "      <td>26577</td>\n",
       "      <td>CTILC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year                                            Line_id  \\\n",
       "0  1926  Discurs llegit per... donar a conèxer la perso...   \n",
       "1  1920  Parlament llegit en la festa inaugural de l'Or...   \n",
       "2  1900                                     Discurs-pròlec   \n",
       "3  1894                                            Discurs   \n",
       "4  1903                                            Discurs   \n",
       "\n",
       "                                                Text  Text_len Source_corpora  \n",
       "0  L'home que per amor al estudi, impulsat per un...     37497          CTILC  \n",
       "1  Cantaires de la Garriga, Senyores i senyors:\\n...      9253          CTILC  \n",
       "2  Discurs-prolec Llegit en la societat mèdic-far...     73881          CTILC  \n",
       "3  Senyors excelentissims, senyors:\\n\\nQuan rebí ...     29393          CTILC  \n",
       "4  Senyors:\\n\\nSembla que era air, y fa ja uns qu...     26577          CTILC  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7a376325-e9eb-4b07-b297-38fb100ef36d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year              0\n",
      "Line_id           0\n",
      "Text              0\n",
      "Text_len          0\n",
      "Source_corpora    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(complete_data.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c2551527-d191-437b-bb7c-6c2293ff6610",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: '2015-10-26'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# sorting joined data by year, from oldest to most recent\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m complete_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYear\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mcomplete_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mYear\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# casting all Years as integers, as we have some strings mixed up\u001b[39;00m\n\u001b[1;32m      3\u001b[0m complete_data\u001b[38;5;241m.\u001b[39msort_values([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYear\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/core/series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4800\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4918\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4922\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m-> 4924\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/core/apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[0;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/core/apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[1;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[1;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/core/base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/core/algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[1;32m   1747\u001b[0m     )\n",
      "File \u001b[0;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[21], line 2\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# sorting joined data by year, from oldest to most recent\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m complete_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYear\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m complete_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYear\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x : \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;66;03m# casting all Years as integers, as we have some strings mixed up\u001b[39;00m\n\u001b[1;32m      3\u001b[0m complete_data\u001b[38;5;241m.\u001b[39msort_values([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYear\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: '2015-10-26'"
     ]
    }
   ],
   "source": [
    "# sorting joined data by year, from oldest to most recent\n",
    "complete_data[\"Year\"] = complete_data[\"Year\"].apply(lambda x : int(x)) # casting all Years as integers, as we have some strings mixed up\n",
    "complete_data.sort_values([\"Year\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e20409-d863-4525-a6c4-dfe3e05458fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resetting index after sorting\n",
    "complete_data = complete_data.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2419ed22-7288-4052-bcd9-17e3f54c0c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickling the complete joined data frame\n",
    "complete_data.to_pickle(\"myfulldata.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8431cf-9875-4148-a453-36520a2820e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a dictionary of the cumulative amount of characters per year\n",
    "year_length = {}\n",
    "# setting year as the index of the dataframe\n",
    "complete_data = complete_data.set_index(\"Year\")\n",
    "# iterating over the rows \n",
    "for year, row in complete_data.iterrows():\n",
    "    if year not in year_length:\n",
    "        year_length[year] = row[\"Text_len\"]  # initializing year's length total\n",
    "    else:\n",
    "        year_length[year] += row[\"Text_len\"]  # accumulating length to already present year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ee90bb-9f2f-462d-905d-19b5da741fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "year_length = dict(sorted(year_length.items(), key=lambda item: item[1]))\n",
    "for key in year_length.keys():\n",
    "    value = year_length[key]\n",
    "    #print(key, \"-\", value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4786c7f6-4ff0-4656-8a74-92c774c05e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker  \n",
    "\n",
    "# placeholder y-values for timeline\n",
    "years = list(year_length.keys())  \n",
    "y_values = np.ones(len(years)) \n",
    "\n",
    "# creating mosaic layout for our multiple plots\n",
    "fig, ax = plt.subplot_mosaic([[\"B\", \"B\"],  # Timeline\n",
    "                              [\"A\", \"A\"],  # Histogram\n",
    "                              [\"D\", \"D\"]],  # Text count & corpus length\n",
    "                             figsize=(10, 8),\n",
    "                             constrained_layout=True)\n",
    "\n",
    "# (A) Histogram of Document Distribution Over Time\n",
    "ax[\"A\"].hist(complete_data.index, bins=min(20, len(years)), histtype=\"step\", color=\"blue\", lw=1.5)\n",
    "ax[\"A\"].ticklabel_format(style='plain', axis='y')\n",
    "ax[\"A\"].set_title(\"Document Distribution Over Time\")\n",
    "ax[\"A\"].set_xlabel(\"Year\")\n",
    "ax[\"A\"].set_ylabel(\"Count\")\n",
    "\n",
    "# (B) Timeline (Scatter Plot)\n",
    "ax[\"B\"].scatter(years, y_values, color=\"blue\", marker=\"o\", lw=1.5)\n",
    "ax[\"B\"].set_title(\"Timeline of Documents\")\n",
    "ax[\"B\"].set_xlabel(\"Year\")\n",
    "ax[\"B\"].set_yticks([])  # Remove y-axis labels since they are not meaningful\n",
    "ax[\"B\"].grid(axis=\"x\")\n",
    "\n",
    "# (D) Text length per year in character counts\n",
    "capped_values = [min(val, 10_000_000) for val in year_length.values()]  # limit to 10M\n",
    "\n",
    "ax[\"D\"].barh(list(year_length.keys()), capped_values, color=\"blue\")\n",
    "ax[\"D\"].set_title(\"Text Length per Year (capped after 10M)\")  \n",
    "ax[\"D\"].set_xlabel(\"Text Length\")  \n",
    "\n",
    "# Fix Tick Labels (Without `ticker`)\n",
    "x_ticks = ax[\"D\"].get_xticks()  \n",
    "ax[\"D\"].set_xticks(x_ticks)  \n",
    "ax[\"D\"].set_xticklabels([f\"{int(x):,}\" for x in x_ticks])  \n",
    "ax[\"D\"].set_xlim(0, 10_000_000)\n",
    "\n",
    "# displaying the plots' mosaic\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
